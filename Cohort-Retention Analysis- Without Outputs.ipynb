{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beac69b9-628a-48fb-9e50-1331662a941e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#E-Commerce Customer Cohort and Retention Analysis\n",
    "\n",
    "#### **Objective:** To analyze customer behavior over time by grouping them into monthly cohorts to identify trends in user retention and churn.\n",
    "\n",
    "##### **Tech Stack:** PySpark (Data Cleaning), Spark SQL (Transformation), Matplotlib/Seaborn (Visualization).\n",
    "\n",
    "##### **Business Impact:** Identifying which months have the highest churn allows marketing teams to target specific user segments for re-engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e13def52-20f9-4c54-a406-87b7ecfd45a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Load the Raw Sales Data into a Spark DataFrame\n",
    "### What this step does:\n",
    "We load the CSV file containing all sales transactions into a Spark DataFrame.\n",
    "\n",
    "header=True tells Spark that the first row contains column names.\n",
    "\n",
    "inferSchema=True automatically detects the correct data types (string, integer, double, etc.).\n",
    "\n",
    "Once loaded, display(df) lets us preview the data and confirm that it loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8185f2e8-e370-4fb1-977b-faec9f41a636",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\n",
    "    \"/Volumes/workspace/default/cohort_data/sales.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96c3e5fd-70d8-474e-9eb6-be68b2a0c202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Inspect the Column Names\n",
    "### What this step does:\n",
    "This prints out all the column names from the DataFrame.\n",
    "It helps you understand the dataset structure, check for messy or unexpected column names, and identify which fields you’ll need to rename or clean before doing any analysis.\n",
    "\n",
    "printSchema() wull print out all the column names and their data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dfe955a-b3a6-401a-9035-a478003c9469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f1080ca-df6a-44cb-b873-7b461cf7ba0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Parse order_date into a Real Date (Handle Mixed Formats)\n",
    "### What this step does:\n",
    "Convert order_date from messy strings (e.g., 1/10/2020, 13-11-2020) into a proper date so Spark can group by month and compute cohorts without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b5fb3e7-eb6a-4b9e-8026-530cd930f427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, date_trunc\n",
    "\n",
    "df_active = (\n",
    "    df\n",
    "    # renames (safe even if not present; but later columns must match)\n",
    "    .withColumnRenamed(\"cust_id\", \"customer_id\")\n",
    "    .withColumnRenamed(\"qty_ordered\", \"quantity\")\n",
    "    .withColumnRenamed(\"price\", \"total_sales\")\n",
    "    .withColumnRenamed(\"Customer Since\", \"customer_since\")\n",
    "\n",
    "    # normalize raw order_date\n",
    "    .withColumn(\"order_raw\", col(\"order_date\").cast(\"string\"))\n",
    "    .withColumn(\"order_raw\", F.regexp_replace(\"order_raw\", r\"\\s+\", \"\"))      # remove spaces\n",
    "    .withColumn(\"order_raw\", F.regexp_replace(\"order_raw\", r\"[./]\", \"-\"))    # turn / or . into -\n",
    "\n",
    "    # parse using ONLY patterns (no try_to_date)\n",
    "    .withColumn(\n",
    "        \"order_date\",\n",
    "        F.coalesce(\n",
    "            F.to_date(\"order_raw\", \"d-M-yyyy\"),    # 13-11-2020\n",
    "            F.to_date(\"order_raw\", \"M-d-yyyy\"),    # 1-10-2020\n",
    "            F.to_date(\"order_raw\", \"d-MMM-yyyy\"),  # 13-Nov-2020\n",
    "            F.to_date(\"order_raw\", \"yyyy-M-d\"),    # 2020-11-13\n",
    "            F.to_date(\"order_raw\", \"yyyy-MM-dd\")   # 2020-11-13\n",
    "        )\n",
    "    )\n",
    "\n",
    "    .filter(F.lower(col(\"status\")).isin(\"complete\", \"received\"))\n",
    "    .withColumn(\"order_month\", F.to_date(date_trunc(\"month\", col(\"order_date\"))))\n",
    "    .drop(\"order_raw\")\n",
    ")\n",
    "\n",
    "display(df_active.select(\"customer_id\", \"order_date\", \"order_month\", \"status\").limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7df58ca1-c4e5-4402-80d6-64ecb88f2637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Identify Each Customer’s First Purchase (Cohort Assignment)\n",
    "### What this step does:\n",
    "This step determines the first time each customer ever made a purchase, which is essential for cohort analysis.\n",
    "\n",
    "We use this first purchase date to assign every customer to a cohort month, meaning the month they became a customer.\n",
    "\n",
    "####Why this matters:\n",
    "- Cohort analysis groups customers by when they first became active.\n",
    "- This allows us to track retention behavior over time and compare how different cohorts perform.\n",
    "####What we do:\n",
    "\n",
    "- Group orders by customer_id\n",
    "- Find the earliest order_date for each customer \n",
    "- Convert that date into a cohort_month by truncating it to the month level \n",
    "- Store cohort_month as a date to ensure consistency in later calculations\n",
    " \n",
    "The resulting table contains one row per customer, representing the cohort they belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73ed4c19-1490-496e-924f-71c76961fe4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, date_trunc, col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, min, date_trunc\n",
    "\n",
    "# Find first order date per customer\n",
    "first_purchase = (\n",
    "    df_active.groupBy(\"customer_id\")\n",
    "    .agg(min(\"order_date\").alias(\"first_order_date\"))\n",
    "    .withColumn(\"cohort_month\", F.to_date(date_trunc(\"month\", col(\"first_order_date\"))))\n",
    ")\n",
    "\n",
    "# Assign cohort month\n",
    "first_purchase = first_purchase.withColumn(\n",
    "    \"cohort_month\",\n",
    "    date_trunc(\"month\", col(\"first_order_date\"))\n",
    ")\n",
    "\n",
    "display(first_purchase.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4061d73-ad11-4eee-b843-362fce7f1fe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Combine Cohort Information With Orders and Calculate Months Since First Purchase\n",
    "### What this step does:\n",
    "This step attaches each customer’s cohort month (their first purchase month) back onto all of their later orders.\n",
    "Then we calculate how many months have passed since the customer’s cohort start date for each order.\n",
    "This is what allows us to track retention month‑by‑month.\n",
    "#### Breakdown:\n",
    "\n",
    "- We join df_active (all valid customer orders) with first_purchase (each customer’s first-ever order date and cohort month).\n",
    "- After the join, every order row contains the customer’s cohort month.\n",
    "- We then use months_between(order_month, cohort_month) to calculate how far along in the retention timeline the customer is:\n",
    "                0 = first month (cohort month)\n",
    " \n",
    "                1 = month after cohort\n",
    " \n",
    "                2 = two months after cohort\n",
    "                \n",
    "                and so on…\n",
    "\n",
    "\n",
    "This field (months_since_cohort) is the core of all retention calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5944830-2fae-4b38-bd21-99da55be3594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, months_between\n",
    "\n",
    "# Join active orders with first_purchase information\n",
    "df_cohort = (\n",
    "    df_active\n",
    "        .join(first_purchase, on=\"customer_id\", how=\"inner\")\n",
    ")\n",
    "\n",
    "# Calculate months since cohort\n",
    "df_cohort = df_cohort.withColumn(\n",
    "    \"months_since_cohort\",\n",
    "    months_between(col(\"order_month\"), col(\"cohort_month\")).cast(\"int\")\n",
    ")\n",
    "\n",
    "display(df_cohort.select(\n",
    "    \"customer_id\",\n",
    "    \"order_month\",\n",
    "    \"cohort_month\",\n",
    "    \"months_since_cohort\"\n",
    ").limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd22b167-67ff-4a35-a028-042558c016fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: Calculate Cohort Size (Number of Customers in Each Cohort)\n",
    "### What this step does:\n",
    "In this step, we determine how many unique customers belong to each cohort.\n",
    "A “cohort” is defined by the month a customer made their first-ever purchase.\n",
    "#### Breakdown:\n",
    "\n",
    "- First, we select only customer_id and cohort_month and remove duplicates so that each customer appears once per cohort.\n",
    "- Then we count how many distinct customers belong to each cohort_month.\n",
    "- This gives us the baseline cohort size, which will act as the denominator when calculating retention rates later.\n",
    "\n",
    "A larger cohort means more customers joined in that month; a smaller cohort means fewer people became new customers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f98cfd95-d173-4d8f-b5ab-67dfd54727ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# One row per customer per cohort\n",
    "customers_per_cohort = (\n",
    "    df_cohort\n",
    "        .select(\"customer_id\", \"cohort_month\")\n",
    "        .dropDuplicates()\n",
    ")\n",
    "\n",
    "# Cohort size table\n",
    "cohort_size = (\n",
    "    customers_per_cohort\n",
    "        .groupBy(\"cohort_month\")\n",
    "        .agg(countDistinct(\"customer_id\").alias(\"cohort_size\"))\n",
    "        .orderBy(\"cohort_month\")\n",
    ")\n",
    "\n",
    "display(cohort_size.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4dbfad9-2112-402f-89fe-c39e50872133",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 7: Count Active Customers by Cohort and Month Index\n",
    "### What this step does:\n",
    "For each cohort and each month after a customer’s first purchase, we count how many distinct customers made a purchase. This gives us the numerator for retention (i.e., “how many came back in month N?”). We first ensure one record per customer per month, then aggregate to get active_customers for every (cohort_month, months_since_cohort) pair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372b970d-b3c1-4636-bd93-3fd2b2b50005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Count distinct customers active in each month since their cohort month\n",
    "active_by_month = (\n",
    "    df_cohort\n",
    "        .select(\"customer_id\", \"cohort_month\", \"months_since_cohort\")\n",
    "        .dropDuplicates()\n",
    "        .groupBy(\"cohort_month\", \"months_since_cohort\")\n",
    "        .agg(countDistinct(\"customer_id\").alias(\"active_customers\"))\n",
    "        .orderBy(\"cohort_month\", \"months_since_cohort\")\n",
    ")\n",
    "\n",
    "display(active_by_month.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aca55e62-0195-40ad-9ba6-315d6d7c2d6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 8: Calculate Retention Rate for Each Cohort and Month\n",
    "### What this step does:\n",
    "Now that we have:\n",
    "\n",
    "cohort_size → how many customers started in each cohort (denominator)\n",
    "\n",
    "active_by_month → how many returned in each month (numerator)\n",
    "\n",
    "We join these two tables and compute the retention rate:\n",
    "retention_rate = active_customers / cohort_size\n",
    "\n",
    "This gives us the percentage of customers who came back in each month since their first purchase.\n",
    "Month 0 should always be 1.0 (100%) because every customer is “active” in their first month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b66d1854-d231-430e-b9c1-532cbf67c649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Join cohort sizes with active customers\n",
    "retention = (\n",
    "    active_by_month\n",
    "        .join(cohort_size, on=\"cohort_month\", how=\"inner\")\n",
    "        .withColumn(\n",
    "            \"retention_rate\",\n",
    "            col(\"active_customers\") / col(\"cohort_size\")\n",
    "        )\n",
    "        .orderBy(\"cohort_month\", \"months_since_cohort\")\n",
    ")\n",
    "\n",
    "display(retention.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "377bb85c-ce47-468b-b75c-2ecaf8fc3cbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 9: Validate Cohort and Order Month Distributions\n",
    "### What this step does:\n",
    "This step helps us double‑check that our cohort creation and monthly order grouping worked correctly. Before calculating retention or building visualizations, we quickly inspect:\n",
    "\n",
    " 1. How many customers are in each cohort month\n",
    "This confirms whether customers are spread across multiple cohorts or concentrated in just a few months.\n",
    "\n",
    "This confirms whether customers are spread across multiple cohorts or concentrated in just a few months.\n",
    "\n",
    "####How many customers per cohort_month?\n",
    "   display\n",
    "   (    \n",
    "    first_purchase.groupBy(\"cohort_month\").count().orderBy(\"cohort_month\")\n",
    "  )\n",
    "\n",
    "2. How many completed orders happened in each order month\n",
    "This shows the volume of active orders per month, helping us understand customer activity and seasonality.\n",
    "\n",
    "####How many active orders per order_month?\n",
    "   \n",
    "   display\n",
    "   (    \n",
    "    df_active.groupBy(\"order_month\").count().orderBy(\"order_month\")\n",
    "   )\n",
    "\n",
    "Why this matters:\n",
    "It ensures that your cohorts look sensible (e.g., not all in one month) and that your monthly order counts align with expected customer behavior. This verification step is helpful before computing retention curves or heatmaps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0af44f45-b0c0-45e2-a92f-8ec39e1b8c78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# How many customers per cohort_month?\n",
    "display(\n",
    "    first_purchase.groupBy(\"cohort_month\").count().orderBy(\"cohort_month\")\n",
    ")\n",
    "\n",
    "# How many active orders per order_month?\n",
    "display(\n",
    "    df_active.groupBy(\"order_month\").count().orderBy(\"order_month\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d80c236-e617-45f2-9449-ba5e6df5c578",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 10: Create a Retention Matrix (Pivot for Heatmap)\n",
    "### What this step does:\n",
    "We limit the analysis to the first 0–12 months after a customer’s first purchase, then pivot the long retention table so each row is a cohort_month and each column is months_since_cohort (0…12). The cell values are the retention rates. This matrix format is ideal for a heatmap visualization, letting you quickly spot drop‑offs and strong‑retention cohorts across months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33023e88-e3ba-4959-91f9-d2a90941ebe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Keep a 0–12 month horizon (common for retention)\n",
    "retention_12 = retention.filter(F.col(\"months_since_cohort\").between(0, 12))\n",
    "\n",
    "# Pivot to matrix form\n",
    "retention_matrix = (\n",
    "    retention_12\n",
    "      .groupBy(\"cohort_month\")\n",
    "      .pivot(\"months_since_cohort\", list(range(0, 13)))\n",
    "      .agg(F.first(\"retention_rate\"))\n",
    "      .orderBy(\"cohort_month\")\n",
    ")\n",
    "\n",
    "display(retention_matrix)  # In Plot options, choose Heatmap (X: months_since, Y: cohort_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f0f7152-af55-440c-8942-b1eed6ab9d1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 11.a: Plot Monthly Retention Curves (One Line per Cohort)\n",
    "### What this step does:\n",
    "We visualize how each cohort’s retention changes over time (months 0–12). We first keep only the first 13 months, convert the Spark DataFrame to a small pandas DataFrame for quick plotting, and then draw a simple line chart where:\n",
    "\n",
    "- X‑axis = months_since_cohort (0, 1, 2, …, 12)\n",
    "- Y‑axis = retention_rate expressed as a percentage\n",
    "- One line per cohort = makes it easy to compare decay patterns across different start months\n",
    "\n",
    "This plot helps you quickly spot common drop‑off points (e.g., Month‑1 cliff) and identify which cohorts retain better or worse than others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dd21be6-5e6f-4b91-aab9-487079aa6e64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert your retention_matrix to pandas\n",
    "plot_df = retention_matrix.toPandas().set_index('cohort_month')\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.heatmap(plot_df, annot=True, fmt=\".0%\", cmap=\"YlGnBu\", cbar_kws={'label': 'Retention Rate'})\n",
    "plt.title('Monthly Cohort Retention Rate (%)')\n",
    "plt.ylabel('Cohort Month')\n",
    "plt.xlabel('Months Since First Purchase')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90681aa2-9b3d-4edd-b263-29a4b6ca168b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 11.b: Forming Matplotlib line chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b658e0f-f4f9-410b-bf9b-f96f9070c1fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Keep a 0–12 month horizon\n",
    "\n",
    "ret_curves = retention.filter(col(\"months_since_cohort\").between(0, 12)) \n",
    "\n",
    "\n",
    "\n",
    "# Convert to pandas for quick plotting\n",
    "\n",
    "pdf = (ret_curves\n",
    "\n",
    "       .orderBy(\"cohort_month\", \"months_since_cohort\")\n",
    "\n",
    "       .toPandas())\n",
    "\n",
    "\n",
    "\n",
    "# Basic line chart: one line per cohort\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for cohort, grp in pdf.groupby(\"cohort_month\"):\n",
    "\n",
    "    plt.plot(grp[\"months_since_cohort\"], grp[\"retention_rate\"] * 100,\n",
    "\n",
    "             label=str(cohort)[:7], alpha=0.85)\n",
    "\n",
    "plt.title(\"Cohort Retention Curves (Monthly)\")\n",
    "\n",
    "plt.xlabel(\"Months since cohort\")\n",
    "\n",
    "plt.ylabel(\"Retention (%)\")\n",
    "\n",
    "plt.grid(alpha=0.25)\n",
    "\n",
    "plt.legend(ncol=2, fontsize=8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51ff2935-3907-4bf9-b827-2738d276a29a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 12: Create KPI Table for Key Retention Milestones (M1, M3, M6, M12)\n",
    "### What this step does:\n",
    "This step builds a compact KPI table that summarizes retention at the most important lifecycle points:\n",
    "\n",
    "- Month 1 (M1): Early retention — shows immediate drop‑off\n",
    "- Month 3 (M3): Medium‑term engagement\n",
    "- Month 6 (M6): Long‑term loyalty\n",
    "- Month 12 (M12): One‑year retention\n",
    "\n",
    "These KPIs help business teams quickly understand how well each cohort is performing without needing to look at the full heatmap or line chart.\n",
    "\n",
    "#### How it works:\n",
    "\n",
    "- Filter retention data to keep only months 1, 3, 6, and 12.\n",
    "- Pivot the numbers so each cohort becomes one row with four columns (M1, M3, M6, M12).\n",
    "- Rename the columns for readability.\n",
    "- Order results by cohort_month.\n",
    "- Display the final KPI table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc38ca96-3587-4667-a99d-a5d18abaeabe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Keep only the milestone months\n",
    "kpis = (\n",
    "    retention\n",
    "      .filter(F.col(\"months_since_cohort\").isin(1, 3, 6, 12))\n",
    "      .groupBy(\"cohort_month\")\n",
    "      .pivot(\"months_since_cohort\", [1, 3, 6, 12])\n",
    "      .agg(F.first(\"retention_rate\"))\n",
    "      .withColumnRenamed(\"1\",  \"M1\")\n",
    "      .withColumnRenamed(\"3\",  \"M3\")\n",
    "      .withColumnRenamed(\"6\",  \"M6\")\n",
    "      .withColumnRenamed(\"12\", \"M12\")\n",
    "      .orderBy(\"cohort_month\")\n",
    ")\n",
    "\n",
    "display(kpis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0098763-1281-40a2-83fd-cce48e76ff87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 13: Persist Outputs as Delta Tables (for Dashboards & Reuse)\n",
    "### What this step does:\n",
    "We save the final analysis tables to a managed Delta database so you can query them later from SQL dashboards (without rerunning the whole notebook) and safely continue work tomorrow.\n",
    "\n",
    "- Creates a lightweight schema (database) named analytics if it doesn’t exist.\n",
    "- Saves three key tables:\n",
    "\n",
    "       analytics.retention_long → long format with cohort_month, months_since_cohort, retention_rate (perfect for heatmaps & line charts).\n",
    "\n",
    "       analytics.retention_matrix → pivoted matrix (rows = cohort months, columns = months since).\n",
    "\n",
    "       analytics.retention_kpis → milestone KPIs (M1, M3, M6, M12) per cohort.\n",
    "\n",
    "These persist in the metastore so you can open Catalog → Workspace → analytics\n",
    " (or run SELECT * FROM WORKSPACE.\n",
    "analytics.<table>) anytime, including in SQL Dashboard tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9913c99-36f5-4d4d-8572-f1b54a3a8583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a lightweight schema to store our outputs\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS analytics\")\n",
    "\n",
    "# Save three handy tables\n",
    "retention.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"analytics.retention_long\")\n",
    "retention_matrix.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"analytics.retention_matrix\")\n",
    "kpis.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"analytics.retention_kpis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "009d5aa3-ce35-4970-8ff9-8f66cf87ff6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 14: Verify That Delta Tables Were Saved Correctly\n",
    "### What this step does:\n",
    "This final step confirms that all the Delta tables we saved (retention_long, retention_matrix, retention_kpis) are correctly stored in the analytics database and can be queried later—especially from SQL Dashboards.\n",
    "It ensures your work is fully persistent and you can safely return tomorrow without re‑running your entire notebook.\n",
    "#### Breakdown:\n",
    "\n",
    "Show all databases to confirm that the analytics schema exists.\n",
    "Show all tables inside analytics to ensure the three Delta tables were created.\n",
    "Preview a few rows from each saved table to verify the content is correct and complete.\n",
    "#### Why this matters:\n",
    "These checks confirm that your results are now permanently stored in Databricks’ catalog, making them reusable for dashboards, future analysis, or continuation of the project without running the entire notebook again.\n",
    "\n",
    "Storing as Delta Tables allows for downstream BI tools like PowerBI or Tableau to connect directly to the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8afaeac7-442e-45f9-bfe3-7894395e9786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List databases (schemas)\n",
    "spark.sql(\"SHOW DATABASES\").show(truncate=False)\n",
    "\n",
    "# List tables in the 'analytics' database\n",
    "spark.sql(\"SHOW TABLES IN analytics\").show(truncate=False)\n",
    "\n",
    "# Preview a few rows from each table\n",
    "display(spark.table(\"analytics.retention_long\").limit(10))\n",
    "display(spark.table(\"analytics.retention_matrix\").limit(10))\n",
    "display(spark.table(\"analytics.retention_kpis\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6adff648-085f-4d13-a534-407314b2dd0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 15: Query the Retention Matrix for Dashboard Visualization (SQL Tile)\n",
    "### What this step does:\n",
    "This SQL query retrieves the full retention matrix table we saved earlier (analytics.retention_matrix).\n",
    "It’s used in your SQL Dashboard tile to generate the cohort retention heatmap.\n",
    "\n",
    "We sort the results by cohort_month so the heatmap shows cohorts in chronological order.\n",
    "\n",
    "#### Why this matters:\n",
    "The retention matrix is the core dataset behind your heatmap visualization.\n",
    "Each row represents a cohort, and each column (0 to 12) represents months since first purchase.\n",
    "This structured layout makes it easy for the dashboard to generate the heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faa179cc-1b44-4d22-9aa5-2dfc34e2357f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM WORKSPACE.analytics.retention_matrix\n",
    "ORDER BY cohort_month;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "482d0a3c-fa28-40b1-8bb8-a50b99716ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## My Key Takeaways from the Analysis: \n",
    "- The highest customer churn occurs within the first month, making early post-purchase engagement the most critical phase for improving retention.\n",
    "- Customers who remain active beyond the first 2–3 months demonstrate stable, long-term behavior, indicating a clear transition from trial users to loyal customers.\n",
    "- Month-1 retention is a strong predictor of long-term customer value, with cohorts performing well early continuing to retain users over time.\n",
    "- Older acquisition cohorts consistently outperform newer cohorts in long-term retention, suggesting a decline in customer quality over time.\n",
    "- Recent cohorts show higher acquisition volume but lower retention, highlighting a trade-off between growth and customer quality.\n",
    "- Retention performance varies significantly across cohorts, reinforcing the importance of cohort-based analysis rather than relying on overall averages.\n",
    "- A small subset of customers drives long-term engagement and value, emphasizing the need to protect and retain high-value users.\n",
    "- Retention follows a predictable decay pattern, enabling more accurate forecasting of customer behavior and revenue.\n",
    "- Certain cohorts consistently underperform across all time periods, indicating ineffective acquisition strategies or external factors such as seasonality.\n",
    "- Cohort-based retention metrics provide a more accurate view of business health than raw user growth, supporting better strategic and investment decisions."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7546971542921924,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Cohort-Retention Analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}